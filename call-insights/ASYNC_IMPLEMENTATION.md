# üöÄ Async Audio Processing Implementation

This document explains how to implement and use the async audio processing system for handling large files efficiently.

## üìã Overview

The async implementation allows you to:
- **Process large files** (>1MB) without blocking the API
- **Submit multiple jobs** concurrently
- **Track progress** in real-time
- **Handle failures** gracefully with automatic rollback
- **Scale horizontally** with multiple workers

## üèóÔ∏è Architecture

```
Client Request ‚Üí FastAPI ‚Üí AsyncProcessor ‚Üí Background Tasks
                     ‚Üì
                Job Queue ‚Üí Worker Pool ‚Üí LangGraph ‚Üí Results
```

## üöÄ Quick Start

### 1. **Start the Server**
```bash
# Development
uvicorn src.Fastapi.main:app --reload --host 0.0.0.0 --port 8000

# Production with Docker
docker-compose up -d
```

### 2. **Process Audio Files**

#### **Synchronous Processing (Small Files)**
```python
import httpx

response = httpx.post("http://localhost:8000/process", json={
    "messages": "Process this audio file",
    "audio_file_key": "small_file.mp3",
    "async_processing": False
})

result = response.json()
print(result["result"])  # Immediate result
```

#### **Asynchronous Processing (Large Files)**
```python
import httpx
import time

# Submit job
response = httpx.post("http://localhost:8000/process", json={
    "messages": "Process this large audio file",
    "audio_file_key": "large_file.mp3",
    "async_processing": True
})

job_id = response.json()["job_id"]
print(f"Job submitted: {job_id}")

# Check status
while True:
    status_response = httpx.get(f"http://localhost:8000/jobs/{job_id}")
    status = status_response.json()
    
    if status["status"] == "completed":
        # Get result
        result_response = httpx.get(f"http://localhost:8000/jobs/{job_id}/result")
        result = result_response.json()
        print(f"Processing complete: {result}")
        break
    elif status["status"] == "failed":
        print(f"Processing failed: {status['error_message']}")
        break
    
    print(f"Progress: {status['progress']}%")
    time.sleep(5)
```

## üìä API Endpoints

### **Core Endpoints**

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/process` | Process audio file (sync or async) |
| `GET` | `/jobs/{job_id}` | Get job status and progress |
| `GET` | `/jobs/{job_id}/result` | Get completed job result |
| `GET` | `/jobs` | List all jobs |
| `DELETE` | `/jobs/{job_id}` | Cancel a job |
| `GET` | `/health` | Health check |

### **Request/Response Examples**

#### **Submit Async Job**
```json
POST /process
{
    "messages": "Process this audio file",
    "audio_file_key": "large_file.mp3",
    "async_processing": true
}

Response:
{
    "job_id": "123e4567-e89b-12d3-a456-426614174000",
    "status": "submitted",
    "message": "Job 123e4567-e89b-12d3-a456-426614174000 submitted for async processing"
}
```

#### **Get Job Status**
```json
GET /jobs/123e4567-e89b-12d3-a456-426614174000

Response:
{
    "job_id": "123e4567-e89b-12d3-a456-426614174000",
    "status": "processing",
    "progress": 45,
    "created_at": "2024-01-15T10:30:00Z",
    "started_at": "2024-01-15T10:30:05Z",
    "completed_at": null,
    "error_message": null,
    "result": null
}
```

## ‚öôÔ∏è Configuration

### **Environment Variables**

```bash
# Required
OPENAI_API_KEY=your_openai_key

# Optional
MAX_CONCURRENT_JOBS=5
LARGE_FILE_THRESHOLD=1000000  # 1MB
MAX_FILE_SIZE=10000000         # 10MB
LAMBDA_TIMEOUT=900            # 15 minutes
LOG_LEVEL=INFO
```

### **File Size Thresholds**

| File Size | Processing Mode | Memory | Timeout |
|-----------|----------------|--------|---------|
| < 1MB | Synchronous | 512MB | 7.5 min |
| > 1MB | Asynchronous | 1024MB | 15 min |

## üîÑ Processing Flow

### **Small Files (< 1MB)**
```
Request ‚Üí Immediate Processing ‚Üí Response
```

### **Large Files (> 1MB)**
```
Request ‚Üí Job Submission ‚Üí Background Processing ‚Üí Status Updates ‚Üí Result Retrieval
```

## üìà Monitoring & Observability

### **Health Check**
```bash
curl http://localhost:8000/health
```

### **Job Monitoring**
```bash
# List all jobs
curl http://localhost:8000/jobs

# Get specific job status
curl http://localhost:8000/jobs/{job_id}
```

### **Logs**
```bash
# View processing logs
docker logs audio-processor

# Follow logs
docker logs -f audio-processor
```

## üöÄ Production Deployment

### **Docker Deployment**
```bash
# Build and run
docker-compose up -d

# Scale workers
docker-compose up -d --scale audio-processor=3
```

### **Kubernetes Deployment**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: audio-processor
spec:
  replicas: 3
  selector:
    matchLabels:
      app: audio-processor
  template:
    metadata:
      labels:
        app: audio-processor
    spec:
      containers:
      - name: audio-processor
        image: your-registry/audio-processor:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: openai-secret
              key: api-key
```

## üîß Advanced Features

### **Batch Processing**
```python
# Submit multiple jobs
job_ids = []
for file_key in file_keys:
    job_id = await processor.process_audio_async(file_key)
    job_ids.append(job_id)

# Wait for all to complete
results = []
for job_id in job_ids:
    result = await processor.wait_for_completion(job_id)
    results.append(result)
```

### **Error Handling**
```python
try:
    result = await processor.wait_for_completion(job_id)
except Exception as e:
    # Handle failure
    print(f"Job failed: {e}")
    # Automatic rollback happens in background
```

### **Custom Configuration**
```python
from src.config import Config

# Override settings
Config.MAX_CONCURRENT_JOBS = 10
Config.LARGE_FILE_THRESHOLD = 500000  # 500KB
```

## üìä Performance Metrics

### **Expected Performance**

| File Size | Processing Time | Cost | Memory Usage |
|-----------|----------------|------|--------------|
| 50KB | 30-60 seconds | $0.005-0.011 | 512MB |
| 1MB | 8-10 minutes | $0.056-0.075 | 1024MB |
| 5MB | 40-50 minutes | $0.28-0.375 | 1024MB |

### **Scaling Considerations**

- **Concurrent Jobs**: 5-10 jobs per worker
- **Memory**: 1GB per worker for large files
- **Storage**: Temporary files cleaned up automatically
- **Database**: Connection pooling recommended

## üõ†Ô∏è Troubleshooting

### **Common Issues**

1. **Job Stuck in Processing**
   ```bash
   # Check logs
   docker logs audio-processor
   
   # Cancel stuck job
   curl -X DELETE http://localhost:8000/jobs/{job_id}
   ```

2. **Memory Issues**
   ```bash
   # Increase memory limit
   export LAMBDA_MEMORY_LARGE=2048
   ```

3. **Timeout Issues**
   ```bash
   # Increase timeout
   export LAMBDA_TIMEOUT=1800  # 30 minutes
   ```

### **Debug Mode**
```bash
# Enable debug logging
export LOG_LEVEL=DEBUG
docker-compose up -d
```

## üéØ Best Practices

1. **File Size Management**
   - Use sync processing for files < 1MB
   - Use async processing for files > 1MB
   - Set appropriate timeouts

2. **Error Handling**
   - Always check job status before retrieving results
   - Implement retry logic for failed jobs
   - Monitor error rates

3. **Resource Management**
   - Limit concurrent jobs per worker
   - Monitor memory usage
   - Clean up temporary files

4. **Security**
   - Use environment variables for secrets
   - Implement rate limiting
   - Validate file types and sizes

## üìö Examples

See `examples/async_client.py` for complete usage examples including:
- Single file processing
- Batch processing
- Error handling
- Progress monitoring
- Result retrieval

## üöÄ Next Steps

1. **Deploy to staging** environment
2. **Test with real data** (1MB+ files)
3. **Monitor performance** metrics
4. **Scale horizontally** as needed
5. **Implement additional features** (chunking, caching, etc.)

The async implementation is production-ready and will handle your 1MB+ files efficiently! üéØ

